%% This is file `elsarticle-template-2-harv.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%%
%% $Id: elsarticle-template-2-harv.tex 155 2009-10-08 05:35:05Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-2-harv.tex $
%%

%%\documentclass[preprint,authoryear,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:

%% Astronomy & Computing uses 5p
%% \documentclass[final,authoryear,5p,times]{elsarticle}
\documentclass[final,authoryear,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
\usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

\usepackage[pdftex,pdfpagemode={UseOutlines},bookmarks,bookmarksopen,colorlinks,linkcolor={blue},citecolor={green},urlcolor={red}]{hyperref}
\usepackage{hypernat}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon (default)
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   authoryear - selects author-year citations (default)
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%   longnamesfirst  -  makes first citation full author list
%%
%% \biboptions{longnamesfirst,comma}

% \biboptions{}

\journal{Astronomy \& Computing}

%% Make single quotes look right in verbatim mode
\usepackage{upquote}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{The Future of Astronomical Data Formats II. Learning from 25
  years of the $N$-Dimensional Data Format}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[cornell]{Tim Jenness\corref{cor1}}
\ead{tjenness@cornell.edu}
\author[jac]{David S. Berry}
\author[jac]{Malcolm J.\ Currie}
\author[noao]{Frossie Economou}
\author[aao]{Keith Shortridge}
\author[bristol]{Mark B.\ Taylor}
\author[ral]{Patrick T.\ Wallace}

\cortext[cor1]{Corresponding author}

\address[cornell]{Department of Astronomy, Cornell University, Ithaca,
  NY 14853, USA}
\address[jac]{Joint Astronomy Centre, 660 N.\ A`oh\=ok\=u Place, Hilo, HI
  96720, USA}
\address[noao]{National Optical Astronomy
  Observatory, 950 N Cherry Ave, Tucson, AZ 85719, USA}
\address[aao]{Australian Astronomical Observatory, 105 Delhi Rd, North
Ryde, NSW 2113, Australia}
\address[bristol]{H.\ H.\ Wills Physics Laboratory, Bristol University, Tyndall Avenue, Bristol, UK}
\address[ral]{Space Science and Technology Department, STFC Rutherford Appleton Laboratory, Harwell Oxford, Didcot, Oxfordshire, OX11 0QX, UK}

\begin{abstract}
%% Text of abstract

The extensible $N$-dimensional Data Format (NDF) was designed and
developed in the late 1980s to provide a data model suitable for use
in a variety of astronomy data processing applications supported by
Starlink. This paper provides an overview of the historical drivers
for the development of NDF and the lessons learned from using the
format for many years in the Starlink software collection and in data
acquisition systems.

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

data formats \sep
Starlink

\end{keyword}

\end{frontmatter}

% \linenumbers

%% Journal abbreviations
\newcommand{\mnras}{Mon Not R Astron Soc}
\newcommand{\aap}{Astron Astrophys}
\newcommand{\aaps}{Astron Astrophys Supp}
\newcommand{\pasp}{Pub Astron Soc Pacific}
\newcommand{\apj}{Astrophys J}
\newcommand{\apjs}{Astrophys J Supp}
\newcommand{\qjras}{Quart J R Astron Soc}
\newcommand{\an}{Astron.\ Nach.}
\newcommand{\ijimw}{Int.\ J.\ Infrared \& Millimeter Waves}
\newcommand{\procspie}{Proc.\ SPIE}
\newcommand{\aspconf}{ASP Conf. Ser.}

%% Applications


%% main text
\section{Introduction}
\label{sec:intro}

{\color{red} Author list not finalised. Should invite PTW, PWD, NXG
  and RFWS and others.}

{\color{red} Introduction is incoherent.}

There is a renewed interest in file format choices for astronomy
including discussions on the future of FITS \citep{P90_adassxxiii} and
with some projects adopting HDF5 and investigating JPEG2000. The
Starlink project \citep{2000ASSL..250...93W,2002A&G....43a..25P},
which began in 1980 \citep{1982MmSAI..53...55T}, was interested in
using a single file format for all their data reduction software. The
shift from a FITS-based format to a hierarchical self-describing
format was driven by data reduction efficiency requirements and the
desire to reuse software. A file format without a corresponding data
model caused chaos and it took sometime until order could be restored
with the definition of the $N$-Dimensional Data Format (NDF).

When the Starlink Project was created it soon became apparent that a
unified file format should be adopted for all the Starlink
applications. FITS \citep{1979ipia.coll..445W,1981A&AS...44..363W} had
recently been developed and was adopted as a tape interchange
format. A file format was required that was explicitly optimized for
data reduction applications and the choice of format involved much
debate (reference to FITS adoption). The initial Starlink software
environment, known as the INTERIM environment, proposed the use of the
Bulk Data Frame \citep[BDF;][]{1980SPIE..264...70P,SUN4}. The BDF format
was heavily influenced by FITS and used a FITS header block with
standard FITS keywords. Whilst it had a flexible design a special
``IMAGE'' variant was standardised for astronomical images and
spectra.

A data reduction file format should be efficient when generating
intermediate files and be able to address and modify individual items.
Additionally, it is import to be able to copy individual
self-contained structures from one file to another. At the time FITS
was not capable of doing this and it was realised that the finalised
Starlink software environment would benefit from a more advanced file
format capable of addressing hierarchical structures
\citep{1981STARENT4}.


\section{Hierarchical Data System}

The Starlink Data System\footnote{from which the file extension of
  \texttt{.sdf}, for Starlink Data File, was chosen.} was first
proposed in 1981 and the first version was released in 1982 \citep[see
e.g.][]{1982QJRAS..23..485D,1991STARB...8....2L}. In 1983 the name was
changed to the Hierarchical Data System (HDS) to make the file format
benefits more explicit. It was originally written in the BLISS
programming language on a VAX/VMS system and later rewritten in C and
ported to Unix.  HDS was in common usage in 1986 although it was
hampered by the next generation Starlink Software Environment
\citep{1986BICDS..30...13L}.  There are many parallels between the
hierarchical way HDS stores data within files and the way that a
filing system organises the files themselves. An early description of
the internal layout of an HDS file can be found in \citet{SSN27a}.

Some key features of the HDS design are:

\begin{itemize}
\item Hierarchical organization of arbitrary structures, including the
  ability to store arrays of structures.
\item The hierarchy is self-describing and can be queried.
\item The ability to associate structures with an arbitrary data type.
\item The ability to delete, copy or rename structures within a file.
\item Automatic byte swapping whilst retaining the native byte order
  for output files.
\end{itemize}

More recently HDS has been extended to support 64-bit file offsets so
that larger files can be written\footnote{Support for individual data
  arrays with a 64-bit integer size is not yet possible.}, and also the
addition of a 64-bit integer data type \citep{P82_adassxxiii}.

\begin{table}
\caption{HDS basic data types. The unsigned types did not correspond
  to standard Fortran~77 data types but had to be included for
  compatibility with astronomy instrumentation.}
\label{tab:hdstypes}
\begin{center}
\begin{tabular}{lll}
\hline
\_BYTE & b & Signed 8-bit integer \\
\_UBYTE & ub & Unsigned 8-bit integer \\
\_WORD & w & Signed 16-bit integer \\
\_UWORD & uw & Unsigned 16-bit integer \\
\_INTEGER & i & Signed 32-bit integer \\
\_INT64 & k &Signed 64-bit integer \\
\_LOGICAL & l & Boolean \\
\_REAL & r & 32-bit IEEE float \\
\_DOUBLE & d & 64-bit IEEE float \\
\_CHAR[$*$n] & c & 8-bit Character string \\
\hline
\end{tabular}
\end{center}
\end{table}

The advantage of HDS over flat file formats is that it allows many different kinds of data to
be stored in a consistent and logical fashion. It is also very
flexible, in that objects can be added or deleted whilst retaining the
logical structure. HDS also provides portability of data, so that the
same data objects may be accessed from different types of computer
despite the fact that each may actually format its files and data in
different ways.

\subsection{\label{sect:objects}HDS Objects}

HDS files are known as \emph{container files} and by default have the
extension `.sdf'.  HDS files contain \emph{data objects} which will
often be referred to simply as \emph{objects}. An object is an entity,
either a scalar or an array,
which contains data or other objects. This is the basis of the
hierarchical nature of HDS and is analogous to the concepts of
\emph{file} and \emph{directory} -- a directory can contain files and
directories which can themselves contain files and directories and so
on.

An HDS object possesses a number of attributes, each of which is
described in more detail below:

\subsubsection{\label{sect:name}Name}

The primary way of identifying an HDS object is by its \emph{name},
which must be unique within its own container object.  The name of an
object is a character string which may contain any printing
characters; white space is ignored and alphabetic characters are
capitalised. The maximum length of an HDS name is 15 characters.

There are no special rules governing the first character
(i.e.\ it can be numeric), so HDS itself allows great freedom in specifying
names (and also types -- see \S\ref{sect:type}). In practice,
however, some restrictions will be imposed by considerations of
portability of data and applications, and of possible syntax conflicts
with the environment within which HDS is used.

\subsubsection{\label{sect:type}Type}

The \emph{type} of an HDS object falls into two \emph{classes} named
Structure and Primitive.

Structure objects contain other objects called
\emph{components}. It is possible to have an array of structures,
each element of which functions as a separate structure.
Primitive objects are only numeric, character,
or logical values. Objects in the different classes are referred to as
\emph{structures} and \emph{primitives} while the more general term
\emph{object} refers to either a \emph{structure} or a
\emph{primitive}. Structures are analogous to the directories in a
filing system -- they can contain a part of the hierarchy below
them. Primitives are analogous to files -- they are at the bottom of
any branch of the structure and contain the actual data.

In HDS, structure types are represented by character strings with the
same rules of formation as \emph{name} (\S\ref{sect:name}) except that
a structure type may not start with an underscore character `\_' (a
structure type may also be completely blank). Examples of structure
types are `IMAGE', `SPECTRUM', `INSTR\_RESP', \emph{etc.} These do not
begin with an underscore, so they are easily distinguished from the
primitive types, which do.

Special rules apply to the primitive types, which all begin with an
underscore and are ``pre-defined'' by HDS, as shown in table
\ref{tab:hdstypes}.

\subsubsection{\label{sect:shape}Shape}

Every HDS object has a \emph{shape} or dimensionality. This is
described by an integer (the number of dimensions) and an integer
array (containing the size of each dimension). A \emph{scalar} (for
example a single number) has, by convention, a dimensionality of zero;
\emph{i.e.}\ its number of dimensions is 0. A \emph{vector} has a
dimensionality of 1; \emph{i.e.}\ its number of dimensions is 1, and
the first element of the dimension array contains the size of the
vector.  An \emph{array} refers to an object with 2 or more dimensions;
a maximum of 7 dimensions are allowed. Objects may be referred to as
\emph{scalar primitives} or \emph{vector structures} and so on.

\subsubsection{State}

The \emph{state} of an HDS object specifies whether or not its value is
defined. It is represented as a logical value where .TRUE.\ means
defined and .FALSE.\ means undefined.

Objects start out undefined when they are created and become defined
when you write a value to them. In general, an error will result if
you attempt to obtain the value of an object while it is still
undefined.


\section{File Format without standardised Data Model}

HDS files allowed people to arrange their data in the files however
they pleased and placed no constraints on the organization of the
structures or the semantic meaning of the content. This resulted in
serious interoperability issues when moving files between applications
that nominally could read HDS files. Within the Starlink ecosystem
there were at least three prominent attempts at providing a data model.
The result was chaos.

\subsection{Wright-Giddings IMAGE}

\begin{figure*}[t]
\begin{minipage}{\textwidth}
\begin{quote}
\small
\begin{verbatim}
HORSEHEAD  <IMAGE>
   DATA_ARRAY(384,512)  <_REAL>   100.5,102.6,110.1,109.8,105.3,107.6,
                                  ... 123.1,117.3,119,120.5,127.3,108.4
   TITLE          <_CHAR*72>      'KAPPA - Flip'
   DATA_MIN       <_REAL>         28.513
   DATA_MAX       <_REAL>         255.94
   AXIS1_DATA     <_REAL}         1,2,3,4,5,6,7,8,
                                  ... 383, 384
   AXIS1_LABEL    <_CHAR*5>       'XAXIS'
\end{verbatim}
\end{quote}
\caption{Example IMAGE structure using the Wright-Giddings
  convention. The components are all at a single level without any hierarchy.}
\end{minipage}
\end{figure*}

An early proposal \citep[][but see also \citet{SGP38}]{WrightGiddings1983} introduced the
\texttt{IMAGE} organizational scheme. This Wright-Giddings design specified that
data should go into a \texttt{DATA\_ARRAY} item and there should also be
items for pre-computed data minimum and maximum, and also a value for
a array-specific blank value. Errors were represented as standard
deviations and stored in \texttt{DATA\_ERROR} and bad-pixel masks were
stored in \texttt{DATA\_QUALITY}. Software was provided to convert BDF
format files to HDS using this model \citep{SUN96} and the format became
reasonably popular because of its simplicity.

There were a number of shortcomings with the design, not the least of
which was that it did not make use of hierarchical structures. The
design was very flat and heavily influenced by FITS and BDF.

\subsection{Figaro DST}

The Figaro data reduction package
\citep[][ascl:1203.013]{1988igbo.conf..448C,1993ASPC...52..219S}
independently adopted a hierarchical design based on HDS. This DST
format\footnote{The reason for the name has been lost in the mists of
  time but our best guess is that it stood for \emph{\textbf{D}ata
    \textbf{ST}ructures}.} made good use of structures and supported
standard deviations for errors. Axis information was stored in
structures labeled X and Y, and the main image/spectral data was
stored in a structure labeled Z. The main data array was
\texttt{.Z.DATA}.  FITS-style keyword/value pairs were encoded
explicitly in a structure called \texttt{.FITS} but using scalar
components for each header item. Any comments associated with the FITS
keywords were held in similar structure labelled
\texttt{.COMMENTS}. This basic structure suggests a bias towards 1- or
2-dimensional data, but it could handle data of up to 6 dimensions;
the \texttt{.Z.DATA} array could have as many dimensions as HDS would
support, and the axis structures for the higher dimensions were
labelled -- awkwardly -- from \texttt{.T} through to \texttt{.W}. An
example trace can be found in Fig.\ \ref{fig:dst}.

\begin{figure*}[t]
\begin{minipage}{\textwidth}
\begin{quote}
\small
\begin{verbatim}
OUT  <FIGARO>
   Z              <IMAGE>         {structure}
      DATA(310,19)   <_REAL>         1655.552,1376.111,1385.559,1746.966,
                                     ... 1513.654,1465.343,1446.902,*,*,*,*,*
      UNITS          <_CHAR*32>      'A/D numbers per exposure'
      LABEL          <_CHAR*32>      'OBJECT - DARK'
      ERRORS(310,19)  <_REAL>        9.330093,4.624712,1.043125,3.801913,
                                     ... 16.92331,11.49692,10.9114,*,*,*,*,*

   OBS            <OBS>           {structure}
      OBJECT         <_CHAR*32>      'krypton singlet'

   X              <AXIS>          {structure}
      DATA(310)      <_REAL>         1.85115,1.852412,1.853675,1.854938,
                                     ... 2.237606,2.238869,2.240132,2.241395
      LABEL          <_CHAR*32>      'Estimated wavelength'
      UNITS          <_CHAR*32>      'microns'

   Y              <AXIS>          {structure}
      DATA(19)       <_REAL>         0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,
                                     ... 12.5,13.5,14.5,15.5,16.5,17.5,18.5

   FITS           <FITS>          {structure}
      INSTRUME       <_CHAR*8>       'CGS4'
      TELESCOP       <_CHAR*8>       'UKIRT'
      SOFTWARE       <_CHAR*8>       'CF v1.0'
\end{verbatim}
\end{quote}
\caption{Partial dump of the structure of a DST file. Structures make
  use of a hierarchy and reuse concepts in the data array and axis
  definition. This example is from a CGS4 observation from January
  1991.}
\label{fig:dst}
\end{minipage}
\end{figure*}

Around 1990, the code used by Figaro to access the DST format was
reworked to handle both DST and NDF format files
\citep{1990STARB...6...18S}. Support for NDF did
not use the actual NDF library; instead it used direct HDS calls for
both formats, but would use different names for the HDS items it
accessed depending on the format of the file. This involved a
significant reworking of the Figaro code, but maintained compatibility
with existing DST format files. However, it has failed to keep up with
recent changes to the NDF format such as support for 64-bit data
files.

\subsection{Asterix}

{\color{red} My examples are from the early 1990s so I'm not sure how
  much of this influenced NDF or was influenced by NDF. Documents from
  the mid 1980s suggest that ASTERIX used an HDS file before NDF and
  were the first to adopt HISTORY.}

The \textsc{Asterix} X-Ray data reduction package \citep{SUN98,1992STARB...9....3S} used the HDS
format exclusively until the introduction of an abstract data access
interface \citep{1995ASPC...77..199A} which allowed for the use of HDS
and FITS format files. \textsc{Asterix} defined its
own data models and layered those on top of HDS. The \texttt{SPECTRA}
layout was closest to that found in NDF and an example is shown in
Fig.\ \ref{fig:asterix} with a trace from a file created in 1992. The
use of \texttt{HISTORY}, \texttt{AXIS} and \texttt{DATA\_ARRAY}
structures provides some similarity to NDF. {\color{red} What
  components did ASTERIX develop themselves?}

\begin{figure*}[t]
\begin{minipage}{\textwidth}
\begin{quote}
\small
\begin{verbatim}
RZ_PSPC_Z00000  <SPECTRA>

   AXIS(3)        <AXIS>          {array of structures}

   Contents of AXIS(1)
      DATA_ARRAY(729)  <_REAL>       0.07115,0.07154,0.07162001,0.07166,
                                     ... 2.97,2.975,2.98,2.985,2.99,2.995,3

   EMBOL_UNITS    <_CHAR*16>      'keV/cm**3/s'
   HISTORY        <HISTORY>       {structure}
      CREATED        <_CHAR*18>      '16-JUN-92 01:07:08'
      RECORDS(10)    <HIST_REC>      {array of structures}

      Contents of RECORDS(1)
         DATE           <_CHAR*30>      '   16-JUN-92 01:07:08'
         COMMAND        <_CHAR*35>      'RS_CREATE V1.3        [1 Jun 92]'

      EXTEND_SIZE    <_INTEGER>      10
      CURRENT_RECORD  <_INTEGER>     2

   DATA_ARRAY(729,2,162)  <_REAL>   10183284,2089223,1044320,1044320,
                                    ... 18350.33,18349.45,18275.11,36474.31
   TMIN           <_REAL>         0.017558
   TMAX           <_REAL>         29.13903
   TLSTEP         <_REAL>         0.02
   ENERGY_BOUNDS(730)  <_REAL>    0.07115,0.07154,0.07162001,0.07166,
                                  ... 2.97,2.975,2.98,2.985,2.99,2.995,3,3.01
   REDSHIFT       <_REAL>         0
   EMBOL(2,162)   <_REAL>         1.2466619E9,3.2856078E10,1.1444687E9,
                                  ... 2.8344144E8,2.3983852E9,2.8333168E8
\end{verbatim}
\end{quote}
\caption{Partial dump of the structure of an ASTERIX HDS
  \texttt{spectra} file. \texttt{HISTORY} and \texttt{AXIS} structures
  make extensive use of hierarchy and arrays of structures.  This
  example comes from the ASTERIX source distribution from 1992.}
\label{fig:asterix}
\end{minipage}
\end{figure*}

{\color{red} Were there any other structures used with HDS other than DST and IMAGE prior to NDF?}

\section{\emph{N}-Dimensional Data Format}

{\color{red}
There is text in SGP/38 that can probably be brought over verbatim.

Funnily enough this seems to be a data model and not a data format.

Interestingly \citet{1993ASPC...52..199B} talks of how useful NDF was in designing the
2dF data reduction system using object-oriented fortran with NDF as
the object backing store.

Not trying to be comprehensive covering all possible astronomy
data. Trying to be generically useful whilst providing an extension facility.

Earliest reference to NDF I have found is Starlink bulletin 1988,
number 2. \citep{1988STARB...2...11C}

Anatomy of an NDF file

Proper block diagram of the hierarchy for each structure type? That
was part of the ADASS poster that could not be squeezed into the ADASS
paper and it needs to be made explicit.
}

\subsection{Data Arrays}

{\color{red}
Data, Variance, Quality

Quality labels.
}
\subsubsection{Data compression}

{\color{red} Short description of options \citep{2008ASPC..394..650C}}

\subsubsection{Chunking}



\subsection{Character Attributes}

Each NDF has three character attributes that can be specified: a
title, a data label and a data unit. These values can be accessed
through the library API without resorting to a FITS-style extension.

\subsection{Axes and WCS}

{\color{red} First there was \texttt{AXIS} and then there was AST. \citep{1998ASPC..145...41W,2001ASPC..238..129B}}

\subsection{History}

{\color{red} Structured with a supporting API.}

\subsection{Extensions}

The NDF standard included a special place for local extensions to the
format. This allowed instruments and applications to track additional
information without requiring standardisation. The only rule was that
each extension should be given a reserved name and that the data type
would define the specific data model of an extension. Some
applications, for example, went so far as to include covariance
information in extensions to overcome limitations in the default error
propagation model for NDF \citep[for example SPECDRE;][]{SUN140}.

Three extensions proved so popular that they are now part of the broader
standard for NDFs.

\subsubsection{FITS headers}

FITS headers, consisting of 80 character header cards, are extremely
common and to simplify interoperability with FITS files and to
minimize structure overhead, it was decided early on to store FITS
headers as an array of 80 character strings matching the FITS
convention rather than attempting to parse the contents and expand
into structures.

\subsubsection{Provenance}

In the late 1980s disk space and processing power were neither so
cheap nor so plentiful as they are now. At the time therefore, it
seemed prudent to limit history propagation to a single ``primary''
parent. Consequently, the NDF library ensures that each application
copies history information from a single primary parent NDF to each
output NDF, appending a new history record to describe the new
application. This means that if many NDFs are combined together by a
network of applications, then each resulting output NDF will, in
general, contain only a subset of the history needed to determine all
the NDFs and applications that were used to create the NDF. It would
of course be possible to gather this information by back-tracking
through all the intermediate NDFs, analysing the history component of
each one, but this depends on the intermediate NDFs still being
available, which is often not the case.

In 2009, it was decided that the inconvenience of this "single line of
descent" approach to history was no longer justified by the savings in
disk space and processing time, and so an alternative system was
provided that enables each NDF to retain full information about all
ancestor NDFs, and the processing that was used to create them. Thus
each NDF may now contain a full ``family tree'' that goes back as far as
any NDFs that have no recorded parents, or which have been marked
explicitly as ``root'' NDFs. Each node in the tree records the name of
the ancestor NDF, the command that was used to create it, the date and
time at which it was created, and its immediate parent nodes. Each
node also allows arbitrary extra information to be associated with the
ancestor NDF. Care is taken to merge nodes -- possibly inherited from
different input NDFs -- that refer to the same ancestor.

For reasons of backward compatibility, it was decided to retain the
original NDF History mechanism, and add this new ``family tree'' feature
as an extra facility named ``Provenance''.  Originally the family tree
was stored as a fully hierarchical structure within an NDF extension,
using raw HDS. However, given the possibility for exponential growth
in the number of ancestors, the cost of navigating such a complex
structure quickly became prohibitive. Therefore, storage as raw HDS
was replaced by an optimised bespoke binary format packed into an
array of integers. Even so, it is still possible for the reading and
writing of provenance information to be a significant overhead when
running relatively minor applications.

{\color{red} Technically an extension rather than part of NDF. Or talk
  in context of history? Explain where NDG fits in and how we extended
  NDF without extending NDF itself (much like .MORE.FITS).}

{\color{red} May be useful if the data structure was explained in an appendix.}

\subsubsection{Quality Labels}

Individual bits in the quality mask can be addressed using the
\texttt{BADBITS} attribute but the NDF standard did not allow for
these bits to be labeled. This confusion was solved first in the
\textsc{iras90} package which added a \texttt{QUALITY\_NAMES}
extension associating names with bits. \textsc{Kappa} tasks were then
modified to understand this convention and allow users to enable and
disable masks by name.

\section{Data Acquisition}

Figaro had a strong influence on the infrared spectroscopy community
in the United Kingdom and the United Kingdom Infrared Telescope
(UKIRT) adopted the DST format for CGS3 and CGS4. NDF was adopted in
1995 although each instrument had a slightly different flavor of
metadata and handling of multiple exposures involved distinct data
files. A unified UKIRT NDF format for all instruments, involving HDS
containers of NDF structures to handle multiple exposures, was
adopted with the release of the ORAC system.

The James Clerk Maxwell Telescope (JCMT) initially used a proposed
submillimeter standard format known as the Global Section Datafile
\citep[GSD;][formerly General Single Dish Data]{sun229}. In 1996 SCUBA
was delivered using the NDF format and a unified NDF raw data format
was adopted for ACSIS and SCUBA-2 from 2006.

{\color{red}AAO? ING?}

\section{Lessons Learned}

\subsection{Successes}

\subsubsection{Hierarchy can be useful}

Initially it was very hard to convince people that hierarchical data
structures were at all useful. This can be seen in the Wright-Giddings
layout of a simple astronomical image file using HDS. Over the years
the adoption of hierarchy has been an important part of the NDF
experience. The ability to relocate or modify parts of a data file,
changing the organization, or to copy related sections to a new file
is very powerful.  NDF extensions containing NDF structures, allowing
software packages to extract those NDFs or simply focus on them
without regard to the enclosing data module becomes very intuitive and
obvious once it is learned. A flat layout would require additional
grouping metadata to understand which components were related and this
is neatly handled by a simple hierarchy.

\subsubsection{Do not try to do everything}

There are two incompatible approaches to designing a data model. The
default position tends to be to think of everything that is needed for
all of astronomy and try to design a model where all metadata is
described and everything has its place. This is the approach taken by
the IVOA \citep[see e.g.][]{2012arXiv1204.3055M} and leads to long and
heated discussions that generate data models that are never quite
perfect and continually need to be tweaked as new instrumentation and
metadata are developed. This is a worthy goal and it is clear that
much can be gained if these data models can be utilised, especially if
related models (spectrum and image when combined into a data cube)
share common ground.

For NDF there was a need to generate a usable model quickly that took
concepts that were generically useful, leaving instrumental details to
extensions. These extensions were ignored by generic software packages
although clear rules were made regarding how extensions should be
propagated. This approach allowed for the format to grow without
requiring the format or the NDF library to understand the
contents of extensions.

This approach has been very successfull, not only in enabling new
features to be added to NDF once the need became obvious, but also
in so far that wavelength regimes not initially involved in the
discussion can make use of NDF without requiring that the core data
model be changed.

\subsubsection{Standardised features aid application writers}

Once application writers understand that there is a standard place for
error information, quality masks and other features of the standard
data model, they begin to write application software that can use
these features. Not having to use a heuristic to determine whether a
particular data array represents an image or an error can allow the
application writer to focus more on the algorithms that matter. Once
users of the software understand that errors and masks are an option
they begin to have an expectation that all the software packages will
handle them. This then motivates the developer to support these
features.  This is especially true if the core concepts of the data
model are simple enough that the learning curve is small.

\subsubsection{Round-tripping to other data formats}

For a new format to be used it must be possible to
convert to and from existing formats in a safe and reliable manner
without losing information.
Significant effort was spent in developing conversion software that
would allow conversion to and from NDF
\citep{SUN55,1997STARB..19...14C}. In particular, special code was
used to recognize specific data models present in FITS files to enable
a more accurate conversion of scientific information into the NDF
form. Support was also added for the \emph{multispec} format in IRAF
\citep{1993ASPC...52..467V} to ensure that wavelength scales were not
lost.

\subsubsection{Adoption of FITS header}

Given hierarchical structures the default assumption might be to use
arrays of keyword structures where each structure would contain the
value, the comment and the unit. Alternatively simply drop the unit
and comment and just use keyword/value pairs. These approaches turned
out to be extremely slow and space inefficient so a pragmatic solution
was taken to standardise on an array of characters formatted
identically to a FITS header. The initial NDF standard did not include
this approach and parsing of a FITS header is left up to the user
rather than being integral to the NDF library. In practice the AST
library \citep{1998ASPC..145...41W} is often used for processing the FITS
header from an NDF so this causes few problems and simplifies the NDF
organization.

\subsubsection{Structure Data Types}

The ability to associate a data type with a structure is important for
the ability of applications to know how to process particular
structures. An NDF extension may itself include many more NDF
structures each of which would have a different name but would also
share the data type of \texttt{NDF}. For example, the GAIA
visualization tool \citep{2009ASPC..411..575D} will scan the HDS file
for all such NDF strutures and make them available for
display. Applications can therefore make decisions on how to handle a
structure without having to know its name.

\subsubsection{Proven ability to enhance format}

The initial NDF design document did not profess to know the future and
was deliberately designed to allow new features to be added as they
became necessary. The adoption of a character array matching an 80
character FITS header was an early change but there have also been
changes to support world coordinate objects
\citep{2001ASPC..238..129B}, data compression algorithms
\citep{2008ASPC..394..650C} and provenance tracking
\citep{2009ASPC..411..418J}. The NDF standard is continuing to evolve
to meet the needs of modern astronomy data processing.

\subsection{Areas for improvement}

\subsubsection{Quality masking}

The initial design for the quality mask used a single unsigned byte
allowing 8 different quality assignments in a single data file. The
design did not allow for the possibility of supporting larger unsigned
integer data types. This restriction should be raised to allow more
assignments. The SMURF map-maker
\citep[][ascl:1310.007]{2013MNRAS.430.2545C} already makes use of more
than 8 internally and uses an unsigned short. When the results are
written out mask types have to be combined if more than 8 were present
in the output data file.

\subsubsection{Table support}

Tables were added to FITS \citep{1988A&AS...73..365H} during the
initial development of NDF and the need for tables got lost in the
drive for a standardised image format. A table type was never added to
NDF and Starlink software eventually took the pragmatic solution of
using FITS binary tables \citep{1995A&AS..113..159C} for output
tables. This can not solve the problem of integrating data tables into
image data files and the format would benefit from a native data
type. The JCMT raw data format instead uses individual 1-dimensional
data arrays to store time-series data but this is inefficient and adds
programming overhead.

{\color{red} Say how FITS2NDF handles binary tables.}

\subsubsection{Flexible variance definitions}

The adoption of variance as a standard part of NDF was an important
motivator for application writers to add support for error propagation and
almost all the Starlink applications now support variance. The next
step is to support different types of errors, including
covariance \citep[see e.g.][]{1992ESOC...41...47M}. This has been proposed many times \citep[see
e.g.][]{1991STARB...8...19M} but is a very difficult problem to solve
in the general case and may involve having to support pluggable
algorithms for handling special types of error propagation.

\subsubsection{Extension complexity}

Many extensions created in the early years of NDF became more complex
than was strictly necessary by ignoring th design decisions of NDF and
using highly complex HDS structures. In many cases it would have been
beneficial to use NDF structures in the extensions which would have
allowed the NDF library to read the data (by giving the full path to
the structure) without resorting to low-level HDS API calls. The NDF
structures would also have been visible to general purpose tools for
visualization.

\subsubsection{Data checksums}

The FITS \texttt{DATASUM} facility is very useful and NDF should support
it. Ideally it should be possible to generate a reference checksum for
a structure. It may be that this has to be done in conjunction with
HDS.

\subsubsection{Character encodings}

The \texttt{\_CHAR} data type in HDS uses an 8-bit character type,
assumed to be ASCII. It was designed long before Unicode came to exist
and has no support for accented characters or non-ASCII character
sets. Multi-byte Unicode should be supported in any modern format to
allow metadata to be represented properly. HDS can not support the
storage of common astronomical units such as $\mu$m or $\AA$.

\subsubsection{Library limitations}

Whilst there are many advantages to having a single library
providing the access layer to an NDF, there is also a related problem
of limitations in this one library causing limitations to all
users. In particular the current NDF library is single-threaded due to
use of a single block of memory tracking access status. Furthermore
the HDS library itself is also single-threaded with its own internal
state. As more and more programs become multi-threaded to make use of
increased numbers of cores in modern CPUs this limitation becomes more
and more frustrating.

Another issue associated with the library is the use of 32-bit
integers as counters in data arrays. It is now easy to imagine data
cubes that exceed this many pixels and a new API may be needed to ease
the transition to 64-bit counters.

Whilst the HDS library is written entirely in ANSI C, the NDF and
related ARY \citep{SUN11} and NDG \citep{SUN2} libraries are written in Fortran.
This can cause a reticence to adoption from people outside the Starlink community and
adds complications when linking in Python and other higher-level
languages.

Ideally NDF would be rewritten in C and be made thread-safe.

\section{Conclusions}

From its beginnings in the mid-1980s the NDF data model has been used
throughout the Starlink software collection within diverse
applications such as SMURF \citep{2013MNRAS.430.2545C}, CCDPACK, GAIA
and KAPPA. It has also been used as a raw data acquisition format at the
James Clerk Maxwell Telescope and the United Kingdom Infrared
Telescope {\color{red} (was it used at ING and AAO?)} and available in that form
from the UKIRT and JCMT archives at the Canadian Astronomy Data Centre
\citep{2008ASPC..394..450E,P01_adassxxiii}. The shift from arbitrary
use of hierarchical structures to a data model enforced by a library
and API is extremely important.

{\color{red} Mention HDX \citep{2003ASPC..295..221G}.}

For the future we are considering the possibility of replacing the HDS
layer with a more widely used hierarchical data format such as HDF5
\citep{Folk:2011:OHT:1966895.1966900}. This would have the advantage
of making NDF available to a much larger community, albeit with NDF
still being in Fortran, and also remove the
need to support HDS in the longer term. NDF and all the existing
Starlink applications would continue to work so long as a conversion
program is made available to convert HDS structures to HDF5 structures.
This would have the added advantage of making it straightforward to
add support for tables natively to the NDF data model. Many of the
concepts in NDF map directly to HDF5. One remaining issue is that HDF5
does not support the notion of arrays of groups so the
\texttt{HISTORY} and \texttt{AXIS} structures in NDF would need to be
remapped into a flatter layout, maybe with numbered components in an
\texttt{AXIS} or \texttt{HISTORY} group.

\section{Acknowledgments}

This research has made use of NASA's Astrophysics Data System.
The Starlink software is currently maintained by the Joint Astronomy
Centre, Hawaii with support from the UK Science and Technology
Facilities Council.

The source code for the NDF library and the Starlink software
(ascl:1110.012) is open-source and is available on github at
\htmladdnormallink{https://github.com/Starlink}.

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%%
%%  \citet{key}  ==>>  Jones et al. (1990)
%%  \citep{key}  ==>>  (Jones et al., 1990)
%%
%% Multiple citations as normal:
%% \citep{key1,key2}         ==>> (Jones et al., 1990; Smith, 1989)
%%                            or  (Jones et al., 1990, 1991)
%%                            or  (Jones et al., 1990a,b)
%% \cite{key} is the equivalent of \citet{key} in author-year mode
%%
%% Full author lists may be forced with \citet* or \citep*, e.g.
%%   \citep*{key}            ==>> (Jones, Baker, and Williams, 1990)
%%
%% Optional notes as:
%%   \citep[chap. 2]{key}    ==>> (Jones et al., 1990, chap. 2)
%%   \citep[e.g.,][]{key}    ==>> (e.g., Jones et al., 1990)
%%   \citep[see][pg. 34]{key}==>> (see Jones et al., 1990, pg. 34)
%%  (Note: in standard LaTeX, only one note is allowed, after the ref.
%%   Here, one note is like the standard, two make pre- and post-notes.)
%%
%%   \citealt{key}          ==>> Jones et al. 1990
%%   \citealt*{key}         ==>> Jones, Baker, and Williams 1990
%%   \citealp{key}          ==>> Jones et al., 1990
%%   \citealp*{key}         ==>> Jones, Baker, and Williams, 1990
%%
%% Additional citation possibilities
%%   \citeauthor{key}       ==>> Jones et al.
%%   \citeauthor*{key}      ==>> Jones, Baker, and Williams
%%   \citeyear{key}         ==>> 1990
%%   \citeyearpar{key}      ==>> (1990)
%%   \citetext{priv. comm.} ==>> (priv. comm.)
%%   \citenum{key}          ==>> 11 [non-superscripted]
%% Note: full author lists depends on whether the bib style supports them;
%%       if not, the abbreviated list is printed even when full requested.
%%
%% For names like della Robbia at the start of a sentence, use
%%   \Citet{dRob98}         ==>> Della Robbia (1998)
%%   \Citep{dRob98}         ==>> (Della Robbia, 1998)
%%   \Citeauthor{dRob98}    ==>> Della Robbia


%% References with bibTeX database:

\bibliographystyle{model2-names-astronomy}
\bibliography{acndf}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model2-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have one of the following forms:
%%   \bibitem[Jones et al.(1990)]{key}...
%%   \bibitem[Jones et al.(1990)Jones, Baker, and Williams]{key}...
%%   \bibitem[Jones et al., 1990]{key}...
%%   \bibitem[\protect\citeauthoryear{Jones, Baker, and Williams}{Jones
%%       et al.}{1990}]{key}...
%%   \bibitem[\protect\citeauthoryear{Jones et al.}{1990}]{key}...
%%   \bibitem[\protect\astroncite{Jones et al.}{1990}]{key}...
%%   \bibitem[\protect\citename{Jones et al., }1990]{key}...
%%   \harvarditem[Jones et al.]{Jones, Baker, and Williams}{1990}{key}...
%%

% \bibitem[ ()]{}

% \end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-2-harv.tex'.
